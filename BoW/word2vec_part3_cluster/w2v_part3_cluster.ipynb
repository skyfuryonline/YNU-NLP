{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for K-means clustering: 81.39464282989502 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "word_vector = model.wv.vectors\n",
    "num_clusters = int(word_vector.shape[0]/5)\n",
    "\n",
    "Kmeans_clustering = KMeans(n_clusters=int(num_clusters))\n",
    "idx = Kmeans_clustering.fit_predict(word_vector)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(\"time taken for K-means clustering:\",elapsed,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.wv.index_to_key,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['owner', 'manager', 'keeper', 'janitor', 'cosmo', 'bartender', 'pa', 'nickname', 'patron', 'proprietor', 'dolphin', 'penthouse']\n",
      "\n",
      "Cluster 1\n",
      "['solar', 'signals']\n",
      "\n",
      "Cluster 2\n",
      "['roast', 'tanya']\n",
      "\n",
      "Cluster 3\n",
      "['amusingly', 'delirious', 'gloriously', 'ott', 'enjoyably', 'sporadically']\n",
      "\n",
      "Cluster 4\n",
      "['dates', 'trips', 'beds', 'showers', 'lingerie', 'bushes', 'rode', 'crushes']\n",
      "\n",
      "Cluster 5\n",
      "['preaching', 'communicating']\n",
      "\n",
      "Cluster 6\n",
      "['cinematic', 'filmmaking', 'cinematographic', 'filmic', 'grandiose', 'chiefly', 'reworking']\n",
      "\n",
      "Cluster 7\n",
      "['secret', 'building', 'lab', 'plant', 'facility', 'laboratory', 'chemical', 'engineering', 'plutonium', 'sanitarium']\n",
      "\n",
      "Cluster 8\n",
      "['claims', 'pretending', 'claiming', 'pretends', 'trusted']\n",
      "\n",
      "Cluster 9\n",
      "['penn', 'connery', 'bean', 'astin', 'bana']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print(\"\\nCluster %d\"%cluster)\n",
    "    words = []\n",
    "    for k,v in word_centroid_map.items():\n",
    "        if v==cluster:\n",
    "            words.append(k)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_bag_of_centroids(wordlist,word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values())+1\n",
    "    bag_of_centroids = np.zeros(num_centroids,dtype=\"float32\")\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word] \n",
    "            bag_of_centroids[index]+=1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/fs4ghgt579d29fb6jsxdrls40000gn/T/ipykernel_27698/3633448914.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as  pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords==True:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review,remove_stopwords=True))\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review,remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_centroids =  np.zeros((train['review'].size,int(num_clusters)),dtype=\"float32\")\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review,word_centroid_map)\n",
    "    counter+=1\n",
    "\n",
    "test_centroids =  np.zeros((test['review'].size,int(num_clusters)),dtype=\"float32\")\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review,word_centroid_map)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame(data={\"Id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
