{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "# print(unlabeled_train.shape)\n",
    "# print(test.shape)\n",
    "# print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords==True:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "# 加载punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=False):\n",
    "    # 返回一个list，分词；strip()去除首尾空格\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))\n",
    "    # 返回值形状：[[第一个分句],[第二个分句],[第三个分句]...]->组成review\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从训练集中解析句子\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/fs4ghgt579d29fb6jsxdrls40000gn/T/ipykernel_4601/3485203933.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n",
      "/var/folders/9c/fs4ghgt579d29fb6jsxdrls40000gn/T/ipykernel_4601/3485203933.py:6: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从未标记的训练集中解析句子\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print(\"从训练集中解析句子\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review,tokenizer)\n",
    "    # 形状：[第一条review,第二条review]，其中每一条review对应:[[...],[...],[...]]见上\n",
    "\n",
    "print(\"从未标记的训练集中解析句子\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/fs4ghgt579d29fb6jsxdrls40000gn/T/ipykernel_4601/3181478921.py:11: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "num_features=300\n",
    "min_words_count =40\n",
    "downsampling=1e-3\n",
    "context= 10\n",
    "num_workers = 4\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"training model...\")\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,vector_size=num_features,sample =downsampling,\\\n",
    "                          window=context,min_count = min_words_count)\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
