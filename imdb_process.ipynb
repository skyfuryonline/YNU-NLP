{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from itertools import chain\n",
    "# 链接可迭代对象\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "embed_size = 300\n",
    "max_len = 512\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0,\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0,\n",
    "                   delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0,\n",
    "                              delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    # if remove_stopwords:\n",
    "    #     stops = set(stopwords.words(\"english\"))\n",
    "    #     words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return (words)\n",
    "\n",
    "\n",
    "def encode_samples(tokenized_samples):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def pad_samples(features, maxlen=max_len, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 22:37:41,946: INFO: running /opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/ipykernel_launcher.py--f=/Users/lihao/Library/Jupyter/runtime/kernel-v3e39542177f754a05c38e4f2d828066672ba3e7c1.json\n",
      "/var/folders/9c/fs4ghgt579d29fb6jsxdrls40000gn/T/ipykernel_99905/3810739897.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review, \"lxml\").get_text()\n"
     ]
    }
   ],
   "source": [
    "program = os.path.basename(sys.argv[0])\n",
    "# 返回路径上最后一个目录/文件名，如xx/xx/x.jpg，返回x.jpg\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "clean_train_reviews, train_labels = [], []\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n",
    "    train_labels.append(train[\"sentiment\"][i])\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(chain(*clean_train_reviews)) | set(chain(*clean_test_reviews))\n",
    "# 25000 25000\n",
    "vocab_size = len(vocab)\n",
    "# 101399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, val_reviews, train_labels, val_labels = train_test_split(clean_train_reviews, train_labels,\n",
    "                                                                        test_size=0.2, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wvmodel_file = os.path.join(\"g:\\\\\", 'lib', 'glove.840B.300d.gensim.txt')\n",
    "wvmodel_file = os.path.join('glove.840B.300d.txt')\n",
    "# wvmodel_file = os.path.join(\"word2vec.txt\")\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(wvmodel_file, binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'\n",
    "\n",
    "train_features = torch.tensor(pad_samples(encode_samples(train_reviews)))\n",
    "val_features = torch.tensor(pad_samples(encode_samples(val_reviews)))\n",
    "test_features = torch.tensor(pad_samples(encode_samples(clean_test_reviews)))\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "weight = torch.zeros(vocab_size + 1, embed_size)\n",
    "for i in range(len(wvmodel.index_to_key)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index_to_key[i]]\n",
    "        print(i)\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index_to_key[i]]]))\n",
    "\n",
    "pickle_file = os.path.join('pickle', 'imdb_glove.pickle3')\n",
    "pickle.dump(\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word, vocab],\n",
    "    open(pickle_file, 'wb'))\n",
    "print('data dumped!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
