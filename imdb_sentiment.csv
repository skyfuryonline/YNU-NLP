表格 1,,,,,,,,,,
模型,private score,Public score,Parameters,,,,,,,
BagofWords,0.8322,0.8322,n_estimators=100,,,,,,,
CNN,0.65292,0.65292,"epoch=10,embed_size=300,num_filter=128,filter_size=3,batch_size=64,lr = 0.8",,,,,,,
LSTM,0.73904,0.73904,"epoch=10,embed_size=300,num_hidden=120,num_layers=2,batch_size=64,lr = 0.05",,,,,,,
attention_LSTM,0.85692,0.85692,"epoch=10,embed_size=300,num_hidden=128,num_layer=2,batch_size=64,lr=0.01",,,,,,,
bert族,,,,,,,,,,
bert_native,0.90312,0.90312,"epoch=3,trainBatch=8,valBatch=16,testBatch=16,AdamW(lr=5e-5)",,,,,,,
deberta_ptuning,0.94588,0.94588,"load_in_4bit=True peft_config{num_virtual_token=20,encoder_hidden_size=128} training_args{num_train_epoch=3,per_device_train_batch_size=2,per_device_eval_batch_size=4,weight_decay=0.01}",,,,,,,
"deberta_LoRa (microsoft/deberta-v3-xsmall)",0.93624,0.93624,"Load_in_4bit=True lora_config{r=16,lora_alpha=32,lora_dropout=0.05} training_args{num_train_epoch=3,per_device_train_batch_size=2,per_device_eval_batch_size=4,weight_decay=0.01}",,,,,,,
"deberta_LoRa (microsoft/deberta-v3-base)",0.95804,0.95804,"max_length=510 lora_config={r=16,lora_alpha=32,lora_dropout=0.05} training_args{num_train_epochs=3,per_device_train_batch_size=2,per_device_eval_batch_size=4,weight_decay=0.01}",,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-base)",0.95256,0.95256,"max_length=510 lora_config{r=16,lora_alpha=32,lora_dropout=0.05} adam-mini={lr=2e-5,betas={0.9,0.999},weight_decay=0.01} training_args{num_train_epoch=3,per_device_train_batch_size=2,per_device_eval_batch_size=4}",,,,,,,
"deberta_LoRa (microsoft/deberta-v3-large)",0.96724,0.96724,"max_length=510 lora_config{r=16,lora_alpha=32,lora_dropout=0.05} training_args{num_train_epoch=3,per_device_train_batch_size=2,per_device_eval_batch_size=4,weight_decay=0.01}",,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-large)",0.95856,0.95856,"load_in_4bit=True max_length =510 lora_config={r=16,lora_alpha=32,lora_dropout=0.05} Adam-mini={lr=2e-5,betas={0.9,0.999},weight_decay=0.01} training_args{num_train_epochs=3,per_device_train_batch_size=1,per_device_eval_batch_size=2,gradient_accumulation_steps=8}",,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-large)",0.96596,0.96596,"max_length =510 lora_config={r=16,lora_alpha=32,lora_dropout=0.05} Adam-mini={lr=2e-5,betas={0.9,0.999},weight_decay=0.01} training_args{num_train_epochs=3,per_device_train_batch_size=2,per_device_eval_batch_size=4}",,,,,,,
"对比：deberta-v2-xlarge/deberta-v2-xlarge-adam-mini 对比：deberta-v3-large/deberta-v3-large-adam-mini",,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v2-xlarge)",kaggle上爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v2-xlarge)",kaggle上爆显存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v2-xlarge) load_in_4bit",kaggle上爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v2-xlarge) load_in_4bit",kaggle上爆显存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v2-xlarge) load_in_4bit gradient_accumulation=8",kaggle上爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v2-xlarge) load_in_4bit gradient_accumulation=8",kaggle上爆内存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v2-xlarge) max_length=510",kaggle爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v2-xlarge) max_length=510",kaggle爆显存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v3-large)",kaggle爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-large)",kaggle爆显存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v3-large) max_length=510 train_batch_size=16 val_batch_size=32",kaggle爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-large) max_length=510 train_batch_size=16 val_batch_size=32",kaggle爆显存,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v3-large) max_length=510 train_batch_size=8 val_batch_size=16",kaggle爆显存,,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-large) max_length=510 train_batch_size=8 val_batch_size=16",kaggle爆显存,,,,,,,,,
测试Adam-mini减少显存的效果：,,,,,,,,,,
"deberta_LoRa (microsoft/deberta-v3-base)","load_in_4bit lora_config={r=16,lora_alpha=32,lora_dropout=0.05} training_args{num_train_epochs=3,per_device_train_batch_size=2,per_device_eval_batch_size=4,weight_decay=0.01} AdamW={lr=2e-5,betas={0.9,0.999},weight_decay={0.01},eps=1e-8}",,,,,,,,,
"deberta_LoRa_adam-mini (microsoft/deberta-v3-base)","load_in_4bit lora_config{r=16,lora_alpha=32,lora_dropout=0.05} training_args{num_train_epoch=3,per_device_train_batch_size=2,per_device_eval_batch_size=4} adam-mini={lr=2e-5,betas={0.9,0.999},weight_decay={0.01}, eps=1e-8,dim=model.config.hidden_size
,n_heads=model.config.num_attention_heads}",,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
SCL和Rdrop,,,,,,,,,,
SupervisedConstrastiveLearning,0.91524,0.91524,"training_args{num_train_epoch=3,per_device_train_batch=2,per_device_eval_batch=4,weight_decay=0.01}",,,,,,,
"SCL+deberta_LoRa (microsoft/deberta-v3-large)",,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
RegularizedDropout,0.93888,0.93888,"training_args{num_train_epoch=3,per_device_train_batch=6,per_device_eval_batch=8,weight_decay=0.01}",,,,,,,
