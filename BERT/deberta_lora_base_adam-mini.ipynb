{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403609a2",
   "metadata": {},
   "source": [
    "## ! pip install datasets\n",
    "! pip install -U evaluate\n",
    "! pip install -U bitsandbytes\n",
    "! pip install wandb\n",
    "! pip install -U transformers\n",
    "! pip install -U huggingface_hub\n",
    "# ! export _HF_DEFAULT_ENDPOINT=https://hf-mirror.com\n",
    "! pip install peft==0.12.0\n",
    "! pip install SentencePiece\n",
    "! pip install adam-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1000. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1000. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskyfurynowonline\u001b[0m (\u001b[33mskyfurynowonline-yunnan-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pod/wandb/run-20241030_132018-f0oyx446</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini/runs/f0oyx446' target=\"_blank\">uncanny-menace-4</a></strong> to <a href='https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini/runs/f0oyx446' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/deberta_lora_base_adam-mini/runs/f0oyx446</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:22<00:00, 902.47 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:05<00:00, 892.99 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:27<00:00, 912.53 examples/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 185,015,044 || trainable%: 0.3196\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "import sys\n",
    "import logging\n",
    "import datasets\n",
    "import evaluate\n",
    "import bitsandbytes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY'] = \"a464ce6c3b972e3e7090ac20839b9a1daac1b608\"\n",
    "\n",
    "from adam_mini import Adam_mini\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, DebertaV2Tokenizer, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     os.environ[\"WANDB_PROJECT\"] = \"deberta_lora_adam-mini\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"deberta_lora_base_adam-mini\"\n",
    "    wandb.init()\n",
    "    \n",
    "    train, val = train_test_split(train, test_size=.2)\n",
    "\n",
    "    train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\n",
    "    val_dict = {'label': val[\"sentiment\"], 'text': val['review']}\n",
    "    test_dict = {\"text\": test['review']}\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_dict)\n",
    "    val_dataset = datasets.Dataset.from_dict(val_dict)\n",
    "    test_dataset = datasets.Dataset.from_dict(test_dict)\n",
    "\n",
    "    # batch_size = 32\n",
    "\n",
    "    model_id = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n",
    "    \n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True,padding=True,max_length=510)\n",
    "    # 尝试加入max_length\n",
    "\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "#         quantization_config = BitsAndBytesConfig(load_in_4bit=True),\n",
    "        # load_in_8bit = True,\n",
    "        # device_map=\"auto\",\n",
    "        # load_in_8bit=True\n",
    "    )\n",
    "    \n",
    "    # Define LoRA Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        # target_modules=['q_proj', 'v_proj'],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    # model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    # add LoRA adaptor\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.20kB [00:00, 5.41MB/s]                   \n",
      "/home/pod/.local/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1565/1057220425.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.0.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.1.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.2.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.3.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.4.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.5.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.6.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.6.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.6.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.6.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.7.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.7.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.7.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.7.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.8.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.8.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.8.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.8.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.9.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.9.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.9.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.9.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.10.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.10.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.10.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.10.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.11.attention.self.query_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.11.attention.self.query_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_A.default.weight torch.Size([16, 768])\n",
      "Adam-mini found the param block with name: base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_B.default.weight torch.Size([768, 16])\n",
      "Adam-mini found the param block with name: base_model.model.classifier.modules_to_save.default.weight torch.Size([2, 768])\n",
      "Adam-mini found the param block with name: base_model.model.classifier.modules_to_save.default.bias torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam-mini found 0 embedding layers, 0 output layers; 0 Querys and Keys;  0 Values;  0 attn_proj;  0 MLPs;\n",
      "=====>>> Warning by Adam-mini: No embedding layer found. If you are training Transformers, please check the name of your embedding layer and manually add them to 'self.embd_names' of Adam-mini. You can do this by adding an additional line of code: optimizer.embd_names.add('the keywords in the name of your embedding layer'). \n",
      "=====>>> Warning by Adam-mini: No output layer found. If you are training Transformers (without weight-tying), please check the name of your output layer and manually add them to 'self.output_names' of Adam-mini. You can do this by adding an additional line of code: optimizer.output_names.add('the keywords in the  name of your output layer').  Please ignore this warning if you are using weight-tying.\n",
      "=====>>>  Warning by Adam-mini: No Query or Key found. If you are training Transformers, please check the name of your Query and Key in attention blocks and manually add them to 'self.wqk_names' of Adam-mini. You can do this by adding two additional lines of code: optimizer.wqk_names.add('the keywords in the  name of your Query' ); optimizer.wqk_names.add('the keywords in the  name of your Key'). \n",
      "=====>>>  Warning by Adam-mini: No Value found. If you are training Transformers, please check the name of your Value in attention blocks and manually add them to 'self.wv_names' of Adam-mini. You can do this by adding an additional lines of code: optimizer.wv_names.add('the keywords in the  name of your Value' ). \n",
      "=====>>>  Warning by Adam-mini: No attn_proj found. If you are training Transformers, please check the name of your attn_proj in attention blocks and manually add them to 'self.attn_proj_names' of Adam-mini. You can do this by adding an additional lines of code: optimizer.attn_proj_names.add('the keywords in the  name of your attn_proj' ). \n",
      "=====>>>  Warning by Adam-mini: No MLP found. If you are training Transformers, please check the name of your MLP in attention blocks and manually add them to 'self.mlp_names' of Adam-mini. You can do this by adding an additional lines of code: optimizer.attn_proj_names.add('the keywords in the  name of your MLP' ). \n",
      "=====>>>  Warning by Adam-mini: you are using default PyTorch partition for Adam-mini. It can cause training instability on large-scale Transformers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='833' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  833/30000 09:04 < 5:18:17, 1.53 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>0.694016</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.693748</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.692672</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.692052</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.691427</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.687974</td>\n",
       "      <td>0.515200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    optimizer = Adam_mini(\n",
    "    named_parameters=model.named_parameters(),\n",
    "    lr=2e-5,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    "    # 其他参数\n",
    "    )\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return  metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./checkpoint',  # output directory\n",
    "#         report_to=\"wandb\",\n",
    "        num_train_epochs=3,  # total number of training epochs\n",
    "        per_device_train_batch_size=2,  # batch size per device during training\n",
    "        per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./logs',  # directory for storing logs\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "#         fp16=True, # 开启混合精度 \n",
    "#         gradient_accumulation_steps=4,# 累积 4 个小批次\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=tokenized_train,  # training dataset\n",
    "        eval_dataset=tokenized_val,  # evaluation dataset\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers = (optimizer,None),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    prediction_outputs = trainer.predict(tokenized_test)\n",
    "    test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n",
    "\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    if not os.path.exists(\"./result\"):\n",
    "        os.mkdir(\"./result\")\n",
    "    result_output.to_csv(\"./result/deberta_lora_base_adam-mini.csv\", index=False, quoting=3)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
